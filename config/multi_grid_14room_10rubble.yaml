{
    # Model configuration
    "MODEL_CONFIG": {
        "custom_model": "actor_critic",
        "custom_model_config": {
            "input_conv_channels": 3,
            "critic_share_layers": False,
            "conv_filters": [
                [32, [2, 2], 1],
                ["pool", [2, 2], 2],
                [64, [2, 2], 1],
                [128, [2, 2], 1],
            ],
            "actor_layer_sizes": [[6272, 7]],
            "critic_layer_sizes": [[6272, 1]],
            "action_masking": True,
        },
    },


    # Environment configuration
    "ENV_CONFIG": {
        "advice_mode": "",
        # icra layout
        "config": [["wwow", "owow", "owof", "owww"], ["wwow", "owow", "ofoo", "owww"], ["wwow", "owow", "ooof", "owww"], ["wwow", "owow", "ofow", "owww"]],
          
        "start_rooms": [[0, 0], [0, 1], [0, 2], [0, 3], 
                        [1, 0], [1, 1], [1, 2], [1, 3], 
                        [2, 0], [2, 1], [2, 2], [2, 3],
                        [3, 0], [3, 1], [3, 2], [3, 3],
                        ],

        # anywhere
        "goal_rooms": [[0, 0], [0, 1], [0, 2], [0, 3], 
                        [1, 0], [1, 1], [1, 2], [1, 3], 
                        [2, 0], [2, 1], [2, 2], [2, 3],
                        [3, 0], [3, 1], [3, 2], [3, 3],
                        ],

        "room_size": 3,
        "max_steps": 500,
        "exploration_bonus": False,

        "num_rubble": 10,
        "rubble_reward": 0.25,
    },


    # Base configuration including algorithm parameters
    "BASE_CONFIG": {
        "env": "multi_grid",
        "alg": "ppo",

        "num_gpus": 1,
        "num_cpus_per_worker": 1,
        "num_cpus_for_driver": 1,
        "framework": "torch", # Only torch supported

        # "lr_schedule": [
        #     [0, 0.002],       # Start with a learning rate of 0.0002
        #     [3000000, 0.0001],  # Reduce to 0.00015 at timestep 20,000
        # ], #--max-steps=50000000
        # "lr": 0.002,
        "lr_schedule": [
            [0, 0.002],       # Start with a learning rate of 0.0002
            [3000000, 0.0001],  # Reduce to 0.00015 at timestep 20,000
        ], #--max-steps=50000000
        "lambda": 0.8,
        "kl_coeff": 0.5,
        "clip_rewards": False,
        "clip_param": 0.2,
        "vf_clip_param": 10.0,
        "vf_loss_coeff": 0.5,
        "entropy_coeff": 0.01, 
        "train_batch_size": 8192, 
        "sgd_minibatch_size": 256,
        "num_sgd_iter": 4,
        "num_workers": 5, 
        "num_envs_per_worker": 5, 
        "batch_mode": "truncate_episodes",
        "observation_filter": "NoFilter",

        "use_gae": True,
        "batch_mode": "truncate_episodes",

        "advice_mode": "never_advise",
        #[adaptive_always_advise, adaptive_decay_advise] 
        #[always_advise, never_advise, decay_advise]
        
        "advice_decay_rate": 0.999996, #0.9999986, #for fixed policies
        "advice_decay_step": 0.0000005, #0.000001,  
        "use_linear_advice_decay": True,
        "follow_teacher_prob": 1.0, 
        "initial_follow_teacher_prob": 0.5,
        "advising_max_step": 1000000, 

        # the teacher model path must be the full path, not relative path. 
        "teacher_model_path": "/home/glow/workspace/aart-hri-repo/adaptive_action_advising/saved_teacher_models/more_rubble/test_model-10rubble_perfect_2024-01-12_01-22-01",
        #"teacher_model_path": "/home/glow/workspace/aart-hri-repo/adaptive_action_advising/saved_teacher_models/more_rubble/test_model-5rubble_2024-01-12_13-19-53",


        "run_inner_loop": True,
        "inner_loop_config": "multi_grid_14room_teacher",
        "inner_loop_frequency": 5000, # in timesteps
        "burn_in": 0,


        "teacher_training_stop_timestep": 100000, 
        "teacher_training_iterations": 150, #100, 

        "adaptive_teacher_advising_period_steps": 30000,
        "adaptive_teacher_initial_advising_rate": 0,  

    },

    #  "HPO_CONFIG": {
    #     "advice_mode": "tune.grid_search([\"decay_advise\", \"never_advise\"])", 
    # }

    

}